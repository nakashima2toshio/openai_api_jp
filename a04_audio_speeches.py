# streamlit run a04_audio_speeches.py --server.port=8504
# --------------------------------------------------
# OpenAI Audio & Speech API ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€åŒ–æ”¹ä¿®ç‰ˆï¼‰
# Streamlitã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªéŸ³å£°APIãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«
# çµ±ä¸€åŒ–æ”¹ä¿®ç‰ˆ: a10_00_responses_api.pyã®æ§‹æˆãƒ»æ§‹é€ ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ»ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®å®Œå…¨çµ±ä¸€
# --------------------------------------------------
import os
import sys
import asyncio
import base64
import time
import random
from io import BytesIO
from pathlib import Path
from abc import ABC, abstractmethod
from typing import Optional, List, Dict, Any, Union
import logging
import textwrap

import streamlit as st

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
BASE_DIR = Path(__file__).resolve().parent.parent
THIS_DIR = Path(__file__).resolve().parent

# PYTHONPATHã«è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
sys.path.insert(0, str(BASE_DIR))

# ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆçµ±ä¸€åŒ–ï¼‰
try:
    from helper_st import (
        UIHelper as BaseUIHelper,
        MessageManagerUI, ResponseProcessorUI,
        SessionStateManager, error_handler_ui, timer_ui,
        InfoPanelManager as BaseInfoPanelManager,
        safe_streamlit_json,
    )
    from helper_api import (
        config, logger, TokenManager, OpenAIClient,
        EasyInputMessageParam, ResponseInputTextParam,
        ConfigManager, MessageManager, sanitize_key,
        error_handler, timer, get_default_messages,
        ResponseProcessor, format_timestamp
    )
except ImportError as e:
    st.error(f"ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.info("å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„: helper_st.py, helper_api.py")
    st.stop()

from openai import OpenAI, AsyncOpenAI
from openai.types.chat import (
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
    ChatCompletionMessageParam,
)

# ãƒšãƒ¼ã‚¸è¨­å®š
def setup_page_config():
    """ãƒšãƒ¼ã‚¸è¨­å®šï¼ˆé‡è¤‡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼å›é¿ï¼‰"""
    try:
        st.set_page_config(
            page_title=config.get("ui.page_title", "OpenAI Audio & Speech API Demo"),
            page_icon=config.get("ui.page_icon", "ğŸµ"),
            layout=config.get("ui.layout", "wide"),
            initial_sidebar_state="expanded"
        )
    except st.errors.StreamlitAPIException:
        pass

setup_page_config()

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
DATA_DIR = Path(config.get("paths.data", "data"))
DATA_DIR.mkdir(exist_ok=True)

# ==================================================
# å…±é€šUIé–¢æ•°ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
def setup_common_ui(demo_name: str) -> str:
    """å…±é€šUIè¨­å®šï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
    safe_key = sanitize_key(demo_name)
    st.write(f"# {demo_name}")

    # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆéŸ³å£°ç”¨ãƒ¢ãƒ‡ãƒ«é¸æŠï¼‰
    model = UIHelper.select_audio_model(f"model_{safe_key}")
    st.write("é¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«:", model)
    return model

def setup_sidebar_panels(selected_model: str):
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ‘ãƒãƒ«ã®çµ±ä¸€è¨­å®šï¼ˆéŸ³å£°APIç”¨ï¼‰"""
    st.sidebar.markdown("### ğŸµ Audio API æƒ…å ±")  # â† expanded=False ã‚’å‰Šé™¤
    InfoPanelManager.show_audio_model_info(selected_model)
    InfoPanelManager.show_session_info()
    InfoPanelManager.show_performance_info()
    InfoPanelManager.show_audio_cost_info(selected_model)
    InfoPanelManager.show_debug_panel()
    InfoPanelManager.show_settings()

# ==================================================
# UIHelperæ‹¡å¼µï¼ˆéŸ³å£°ç”¨ï¼‰
# ==================================================
class UIHelper(BaseUIHelper):
    """UIHelperæ‹¡å¼µï¼ˆéŸ³å£°APIç”¨ï¼‰"""

    @staticmethod
    def select_audio_model(key: str = "audio_model_selection") -> str:
        """éŸ³å£°APIç”¨ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
        audio_models = config.get("models.categories.audio", [
            "tts-1", "tts-1-hd", "gpt-4o-mini-tts",
            "whisper-1", "gpt-4o-transcribe"
        ])
        default_model = config.get("models.audio_default", "tts-1")
        default_index = audio_models.index(default_model) if default_model in audio_models else 0

        selected = st.sidebar.selectbox(
            "éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
            audio_models,
            index=default_index,
            key=key,
            help="åˆ©ç”¨ã™ã‚‹OpenAIéŸ³å£°ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
        )
        SessionStateManager.set_user_preference("selected_audio_model", selected)
        return selected

    @staticmethod
    def select_voice(key: str = "voice_selection") -> str:
        """éŸ³å£°é¸æŠUI"""
        voices = config.get("audio.voices", ["alloy", "nova", "echo", "onyx", "shimmer"])
        return st.selectbox(
            "Voice",
            voices,
            index=0,
            key=key,
            help="éŸ³å£°ã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )

    @staticmethod
    def create_audio_download_button(audio_data: bytes, filename: str, label: str = "ğŸ“¥ éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
        """éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®ä½œæˆ"""
        try:
            st.download_button(
                label=label,
                data=audio_data,
                file_name=filename,
                mime="audio/mp3",
                help=f"{filename}ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™"
            )
        except Exception as e:
            st.error(f"éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Audio download button error: {e}")

# ==================================================
# InfoPanelManageræ‹¡å¼µï¼ˆéŸ³å£°ç”¨ï¼‰
# ==================================================
class InfoPanelManager(BaseInfoPanelManager):
    """InfoPanelManageræ‹¡å¼µï¼ˆéŸ³å£°APIç”¨ï¼‰"""

    @staticmethod
    def show_audio_model_info(selected_model: str):
        with st.sidebar.expander("ğŸµ éŸ³å£°ãƒ¢ãƒ‡ãƒ«æƒ…å ±", expanded=False):
            if selected_model.startswith("tts-") or selected_model.startswith("gpt-4o-mini-tts"):
                st.info("ğŸ¤ Text-to-Speech ãƒ¢ãƒ‡ãƒ«")
                model_info = {
                    "tts-1": "é€Ÿåº¦æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«",
                    "tts-1-hd": "é«˜å“è³ªãƒ¢ãƒ‡ãƒ«",
                    "gpt-4o-mini-tts": "GPT-4oæ­è¼‰TTS"
                }
                st.write(f"**ç‰¹å¾´**: {model_info.get(selected_model, 'ä¸æ˜')}")
                st.write("**å‡ºåŠ›**: MP3")
                st.write("**æœ€å¤§æ–‡å­—æ•°**: 4,096æ–‡å­—")
            else:
                st.info("ğŸ§ Speech-to-Text ãƒ¢ãƒ‡ãƒ«")
                model_info = {
                    "whisper-1": "æ¨™æº–éŸ³å£°èªè­˜",
                    "gpt-4o-transcribe": "é«˜ç²¾åº¦éŸ³å£°èªè­˜"
                }
                st.write(f"**ç‰¹å¾´**: {model_info.get(selected_model, 'ä¸æ˜')}")
                st.write("**å…¥åŠ›**: MP3, WAV, M4A")
                st.write("**æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: 25MB")

    @staticmethod
    def show_audio_cost_info(selected_model: str):
        with st.sidebar.expander("ğŸ’° éŸ³å£°APIæ–™é‡‘", expanded=False):
            if "tts" in selected_model:
                st.write("**TTSæ–™é‡‘ (1000æ–‡å­—ã‚ãŸã‚Š)**")
                tts_pricing = {"tts-1": "$0.015", "tts-1-hd": "$0.030", "gpt-4o-mini-tts": "$0.025"}
                current_price = tts_pricing.get(selected_model, "ä¸æ˜")
                st.write(f"- {selected_model}: {current_price}")
                char_count = st.number_input("æ–‡å­—æ•°", min_value=1, value=100, step=10, key="tts_char_count")
                if selected_model in tts_pricing:
                    price = float(tts_pricing[selected_model].replace("$", ""))
                    cost = (char_count / 1000) * price
                    st.write(f"**æ¨å®šã‚³ã‚¹ãƒˆ**: ${cost:.6f}")
            else:
                st.write("**STTæ–™é‡‘ (åˆ†ã‚ãŸã‚Š)**")
                st.write("- whisper-1: $0.006")
                st.write("- gpt-4o-transcribe: $0.010")
                duration_minutes = st.number_input("éŸ³å£°æ™‚é–“ï¼ˆåˆ†ï¼‰", min_value=0.1, value=1.0, step=0.1, key="stt_duration")
                stt_pricing = {"whisper-1": 0.006, "gpt-4o-transcribe": 0.010}
                if selected_model in stt_pricing:
                    cost = duration_minutes * stt_pricing[selected_model]
                    st.write(f"**æ¨å®šã‚³ã‚¹ãƒˆ**: ${cost:.6f}")

# ==================================================
# åŸºåº•ã‚¯ãƒ©ã‚¹ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
class BaseDemo(ABC):
    """ãƒ‡ãƒ¢æ©Ÿèƒ½ã®åŸºåº•ã‚¯ãƒ©ã‚¹ï¼ˆéŸ³å£°ç”¨çµ±ä¸€åŒ–ç‰ˆï¼‰"""

    def __init__(self, demo_name: str):
        self.demo_name = demo_name
        self.config = ConfigManager("config.yml")
        try:
            self.client = OpenAIClient()
            self.openai_client = OpenAI()
            self.async_client = AsyncOpenAI()
        except Exception as e:
            st.error(f"OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.stop()

        self.safe_key = sanitize_key(demo_name)
        self.message_manager = MessageManagerUI(f"messages_{self.safe_key}")
        self.model = None

        SessionStateManager.init_session_state()
        self._initialize_session_state()

    def _initialize_session_state(self):
        session_key = f"demo_state_{self.safe_key}"
        if session_key not in st.session_state:
            st.session_state[session_key] = {
                'initialized': True,
                'model': self.config.get("models.audio_default", "tts-1"),
                'execution_count': 0,
                'last_audio_file': None
            }

    def get_model(self) -> str:
        return st.session_state.get(f"model_{self.safe_key}", config.get("models.audio_default", "tts-1"))

    def initialize(self):
        self.model = setup_common_ui(self.demo_name)
        setup_sidebar_panels(self.model)

    def handle_error(self, e: Exception):
        lang = config.get("i18n.default_language", "ja")
        error_msg = config.get(f"error_messages.{lang}.general_error", "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        st.error(f"{error_msg}: {str(e)}")
        if config.get("experimental.debug_mode", False):
            with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=False):
                st.exception(e)

    @error_handler_ui
    @timer_ui
    def safe_audio_api_call(self, api_func: callable, *args, **kwargs):
        max_retries = config.get("api.max_retries", 3)
        delay = config.get("api.retry_delay", 1)
        for attempt in range(max_retries):
            try:
                return api_func(*args, **kwargs)
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                logger.warning(f"éŸ³å£°APIå‘¼ã³å‡ºã—å¤±æ•— (è©¦è¡Œ {attempt + 1}/{max_retries}): {e}")
                time.sleep(delay)
                delay *= 2 + random.random()

    @abstractmethod
    def run(self):
        pass

# ==================================================
# Text to Speech ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
class TextToSpeechDemo(BaseDemo):
    """Text to Speech API ã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        self.initialize()

        with st.expander("OpenAI Audio API:å®Ÿè£…ä¾‹", expanded=False):
            st.write("ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã™ã‚‹TTSãƒ‡ãƒ¢ã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¿å­˜ã§MP3å‡ºåŠ›ã€‚")
            st.code(textwrap.dedent("""\
        # Text to Speech API ä½¿ç”¨ä¾‹
        with client.audio.speech.with_streaming_response.create(
            model=model, voice=voice, input=text
        ) as response:
            response.stream_to_file(output_path)
        # ä¿å­˜ã—ãŸMP3ã‚’å†ç”Ÿ
        st.audio(str(output_path), format="audio/mp3")
            """), language="python")

        col1, col2 = st.columns([2, 1])
        with col1:
            voice = UIHelper.select_voice(f"voice_{self.safe_key}")
        with col2:
            stream = st.checkbox("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è»¢é€", help="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ç”Ÿæˆ", key=f"stream_{self.safe_key}", value=True)

        input_method = st.radio("å…¥åŠ›æ–¹æ³•", ["ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å…¥åŠ›", "txtãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ"], key=f"input_method_{self.safe_key}")
        if input_method == "ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å…¥åŠ›":
            self._run_direct_text_input(voice, stream)
        else:
            self._run_file_input(voice, stream)

    def _run_direct_text_input(self, voice: str, stream: bool):
        example_text = config.get("samples.audio.tts_example", "ã“ã‚“ã«ã¡ã¯ã€ã“ã‚Œã¯ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚")
        with st.form(key=f"tts_form_{self.safe_key}"):
            text_input = st.text_area("å¤‰æ›ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
                                      height=config.get("ui.text_area_height", 100),
                                      placeholder=example_text)
            submitted = st.form_submit_button("ğŸµ éŸ³å£°ç”Ÿæˆ")
        if submitted and text_input:
            self._process_tts(text_input, voice, stream, "direct_input")

    def _run_file_input(self, voice: str, stream: bool):
        txt_files = sorted(DATA_DIR.glob("*.txt"))
        if not txt_files:
            st.warning(f"{DATA_DIR} ã« .txt ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            st.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„")
            return
        txt_name = st.selectbox("ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
                                [f.name for f in txt_files],
                                key=f"txt_select_{self.safe_key}")
        txt_file = DATA_DIR / txt_name
        if st.button(f"ğŸµ {txt_name} ã‚’éŸ³å£°å¤‰æ›", key=f"convert_{self.safe_key}"):
            try:
                text = txt_file.read_text(encoding="utf-8")
                self._process_tts(text, voice, stream, txt_name)
            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def _process_tts(self, text: str, voice: str, stream: bool, source: str):
        try:
            session_key = f"demo_state_{self.safe_key}"
            if session_key in st.session_state:
                st.session_state[session_key]['execution_count'] += 1

            max_chars = config.get("audio.tts_max_chars", 4096)
            if len(text) > max_chars:
                st.warning(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã¾ã™ï¼ˆ{len(text)}æ–‡å­— > {max_chars}æ–‡å­—åˆ¶é™ï¼‰")
                text = text[:max_chars]
                st.info(f"ãƒ†ã‚­ã‚¹ãƒˆã‚’{max_chars}æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã¾ã—ãŸ")

            ts = int(time.time())
            suffix = "_stream" if stream else ""
            output_name = f"tts_{source}_{ts}{suffix}.mp3"
            output_path = DATA_DIR / output_name

            with st.spinner("ğŸµ éŸ³å£°ç”Ÿæˆä¸­..."):
                if stream:
                    def tts_stream_call():
                        return self.openai_client.audio.speech.with_streaming_response.create(
                            model=self.model,
                            voice=voice,
                            input=text,
                        )
                    with self.safe_audio_api_call(tts_stream_call) as response:
                        response.stream_to_file(output_path)
                else:
                    def tts_call():
                        return self.openai_client.audio.speech.create(
                            model=self.model,
                            voice=voice,
                            input=text,
                        )
                    resp = self.safe_audio_api_call(tts_call)
                    output_path.write_bytes(resp.content)

            st.success("âœ… éŸ³å£°ç”Ÿæˆå®Œäº†")
            st.audio(str(output_path), format="audio/mp3")
            with open(output_path, "rb") as f:
                UIHelper.create_audio_download_button(f.read(), output_name)

            st.session_state[session_key]['last_audio_file'] = str(output_path)
        except Exception as e:
            self.handle_error(e)

# ==================================================
# Speech to Text ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
class SpeechToTextDemo(BaseDemo):
    """Speech to Text API ã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        self.initialize()

        with st.expander("OpenAI Audio API:å®Ÿè£…ä¾‹", expanded=False):
            st.write("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹STTãƒ‡ãƒ¢ã€‚")
            st.code(textwrap.dedent("""\
                # Speech to Text API ä½¿ç”¨ä¾‹
                with open(audio_file, "rb") as f:
                    transcript = client.audio.transcriptions.create(
                        model=model,
                        file=f,
                        response_format="text",
                    )
                st.write(transcript)
            """), language="python")

        audio_files = self._get_audio_files()
        if not audio_files:
            st.warning(f"{DATA_DIR} ã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«(.mp3, .wav)ãŒã‚ã‚Šã¾ã›ã‚“")
            st.info("TTSãƒ‡ãƒ¢ã§ç”Ÿæˆã—ãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
            return

        selected_file = st.selectbox("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
                                     [f.name for f in audio_files],
                                     key=f"audio_select_{self.safe_key}")
        audio_file_path = DATA_DIR / selected_file
        st.audio(str(audio_file_path), format="audio/mp3")

        stt_model = st.selectbox("STTãƒ¢ãƒ‡ãƒ«", ["whisper-1", "gpt-4o-transcribe"], key=f"stt_model_{self.safe_key}")
        if st.button("ğŸ¤ æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ", key=f"transcribe_{self.safe_key}"):
            self._process_transcription(audio_file_path, selected_file, stt_model)

        self._display_transcription_result(selected_file)

    def _get_audio_files(self) -> List[Path]:
        patterns = config.get("audio.supported_formats", ["*.mp3", "*.wav", "*.m4a"])
        files: List[Path] = []
        for pattern in patterns:
            files.extend(DATA_DIR.glob(pattern))
        return sorted(files)

    def _process_transcription(self, audio_file: Path, filename: str, stt_model: str):
        try:
            session_key = f"transcription_{filename}"
            with st.spinner("ğŸ¤ æ–‡å­—èµ·ã“ã—ä¸­..."):
                with audio_file.open("rb") as f:
                    def transcribe_call():
                        return self.openai_client.audio.transcriptions.create(
                            model=stt_model,
                            file=f,
                            response_format="text",
                        )
                    result = self.safe_audio_api_call(transcribe_call)

            text = result if isinstance(result, str) else getattr(result, "text", str(result))
            st.session_state[session_key] = {
                "transcript": text,
                "model": stt_model,
                "timestamp": time.time(),
                "filename": filename,
            }
            st.success("âœ… æ–‡å­—èµ·ã“ã—å®Œäº†")
            st.rerun()
        except Exception as e:
            self.handle_error(e)

    def _display_transcription_result(self, filename: str):
        session_key = f"transcription_{filename}"
        if session_key in st.session_state:
            result = st.session_state[session_key]
            st.subheader("ğŸ“ æ–‡å­—èµ·ã“ã—çµæœ")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«**: {result['filename']}")
            with col2:
                st.write(f"**ãƒ¢ãƒ‡ãƒ«**: {result['model']}")
            with col3:
                timestamp = format_timestamp(result['timestamp'])
                st.write(f"**å‡¦ç†æ™‚åˆ»**: {timestamp}")
            transcript = result['transcript']
            st.write("**å†…å®¹**:")
            st.markdown(f"> {transcript}")
            c1, c2 = st.columns(2)
            with c1:
                if st.button("ğŸ“‹ ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ”ãƒ¼", key=f"copy_transcript_{filename}"):
                    st.write("ğŸ“‹ ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸï¼ˆæ‰‹å‹•ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ï¼‰")
                    st.code(transcript)
            with c2:
                UIHelper.create_audio_download_button(
                    transcript.encode("utf-8"),
                    f"transcript_{filename}_{int(result['timestamp'])}.txt",
                    "ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                )

# ==================================================
# éŸ³å£°ç¿»è¨³ãƒ‡ãƒ¢ï¼ˆè‹±è¨³ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
# ==================================================
class SpeechTranslationDemo(BaseDemo):
    """Speech Translation API ã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        self.initialize()

        with st.expander("OpenAI Audio API:å®Ÿè£…ä¾‹", expanded=False):
            st.write("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‹±èªãƒ†ã‚­ã‚¹ãƒˆã«ç¿»è¨³ã€‚è‹±è¨³ã•ã‚Œãªã„å ´åˆã¯Chatã§è‹±è¨³ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚")
            st.code(textwrap.dedent("""\
                # Speech Translation API ä½¿ç”¨ä¾‹
                with open(audio_file, "rb") as f:
                    translation = client.audio.translations.create(
                        model="whisper-1",
                        file=f,
                        response_format="text",
                    )
                # ä¸‡ä¸€è‹±èªåŒ–ã•ã‚Œãªã„ã¨ãã¯ Chat ã§è‹±è¨³ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            """), language="python")

        audio_files = self._get_audio_files()
        if not audio_files:
            st.warning(f"{DATA_DIR} ã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        selected_file = st.selectbox("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
                                     [f.name for f in audio_files],
                                     key=f"audio_select_{self.safe_key}")
        audio_file_path = DATA_DIR / selected_file
        st.audio(str(audio_file_path), format="audio/mp3")

        tr_model = st.selectbox("ç¿»è¨³ç”¨ã®éŸ³å£°ãƒ¢ãƒ‡ãƒ«ï¼ˆã¾ãšã¯ã“ã“ã§è‹±è¨³ã‚’è©¦è¡Œï¼‰",
                                ["whisper-1", "gpt-4o-transcribe"],
                                key=f"tr_model_{self.safe_key}")

        if st.button("ğŸŒ è‹±èªç¿»è¨³å®Ÿè¡Œ", key=f"translate_{self.safe_key}"):
            self._process_translation(audio_file_path, selected_file, tr_model)

        self._display_translation_result(selected_file)

    def _get_audio_files(self) -> List[Path]:
        patterns = config.get("audio.supported_formats", ["*.mp3", "*.wav", "*.m4a"])
        files: List[Path] = []
        for pattern in patterns:
            files.extend(DATA_DIR.glob(pattern))
        return sorted(files)

    def _process_translation(self, audio_file: Path, filename: str, tr_model: str):
        try:
            session_key = f"translation_{filename}"
            with st.spinner("ğŸŒ è‹±èªç¿»è¨³ä¸­..."):
                with audio_file.open("rb") as f:
                    if tr_model == "whisper-1":
                        def translate_call():
                            return self.openai_client.audio.translations.create(
                                model="whisper-1",
                                file=f,
                                response_format="text",
                            )
                        raw = self.safe_audio_api_call(translate_call)
                        first_text = raw if isinstance(raw, str) else getattr(raw, "text", str(raw))
                    else:
                        def transcribe_call():
                            return self.openai_client.audio.transcriptions.create(
                                model=tr_model,
                                file=f,
                                response_format="text",
                            )
                        raw = self.safe_audio_api_call(transcribe_call)
                        first_text = raw if isinstance(raw, str) else getattr(raw, "text", str(raw))

                english_text = self._force_translate_to_english(first_text)

            st.session_state[session_key] = {
                "translation": english_text,
                "model": tr_model,
                "timestamp": time.time(),
                "filename": filename,
            }
            st.success("âœ… ç¿»è¨³å®Œäº†")
            st.rerun()
        except Exception as e:
            self.handle_error(e)

    def _force_translate_to_english(self, source_text: str) -> str:
        """
        Chat ã‚’ä½¿ã£ã¦è‹±èªã«å¼·åˆ¶å¤‰æ›ã€‚
        ã™ã§ã«è‹±èªãªã‚‰ãã®ã¾ã¾è¿”ã™ / èª¬æ˜ã¯è¿”ã•ãªã„
        """
        sys_prompt = (
            "You are a professional translator. "
            "If the user's text is not English, translate it into natural, fluent English. "
            "If it is already English, return it unchanged. "
            "Preserve names and style. Return only the English text without explanations."
        )
        try:
            resp = self.openai_client.chat.completions.create(
                model=config.get("models.translate_fallback", "gpt-4o-mini"),
                temperature=0,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": source_text},
                ],
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Translate fallback failed: {e}")
            return source_text

    def _display_translation_result(self, filename: str):
        session_key = f"translation_{filename}"
        if session_key in st.session_state:
            result = st.session_state[session_key]
            st.subheader("ğŸŒ è‹±èªç¿»è¨³çµæœ")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«**: {result['filename']}")
            with col2:
                st.write(f"**ãƒ¢ãƒ‡ãƒ«**: {result['model']}")
            with col3:
                timestamp = format_timestamp(result['timestamp'])
                st.write(f"**å‡¦ç†æ™‚åˆ»**: {timestamp}")
            translation = result["translation"]
            st.write("**English**:")
            st.markdown(f"> {translation}")
            c1, c2 = st.columns(2)
            with c1:
                if st.button("ğŸ“‹ ç¿»è¨³ã‚³ãƒ”ãƒ¼", key=f"copy_translation_{filename}"):
                    st.write("ğŸ“‹ ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸï¼ˆæ‰‹å‹•ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ï¼‰")
                    st.code(translation)
            with c2:
                st.download_button(
                    "ğŸ“¥ ç¿»è¨³ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=translation,
                    file_name=f"translation_{filename}_{int(result['timestamp'])}.txt",
                    mime="text/plain",
                )

# ==================================================
# Realtime API ãƒ‡ãƒ¢
# ==================================================
class RealtimeApiDemo(BaseDemo):
    """Realtime API ã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        self.initialize()

        with st.expander("OpenAI Realtime API:å®Ÿè£…ä¾‹", expanded=False):
            st.write("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°å¯¾è©±ãƒ‡ãƒ¢ï¼ˆè¦ pyaudio/simpleaudioï¼‰")
            st.code('''# Realtime APIä½¿ç”¨ä¾‹
async with async_client.beta.realtime.connect(model="gpt-4o-realtime-preview") as conn:
    await conn.session.update(session={
        "voice": "alloy",
        "input_audio_format": "pcm16",
        "turn_detection": {"type": "server_vad"},
    })
# ãƒã‚¤ã‚¯â†’WebSocketâ†’éŸ³å£°å‡ºåŠ›ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†''', language="python")

        st.warning("âš ï¸ ã“ã®ãƒ‡ãƒ¢ã¯å®Ÿé¨“çš„æ©Ÿèƒ½ã§ã™ã€‚ãƒã‚¤ã‚¯ã¨ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ãŒå¿…è¦ã§ã™ã€‚")

        if not self._check_dependencies():
            return

        self._show_settings()

        if st.button("ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±é–‹å§‹", key=f"realtime_{self.safe_key}"):
            self._run_realtime_demo()

    def _check_dependencies(self) -> bool:
        missing = []
        try:
            import pyaudio  # noqa
        except ImportError:
            missing.append("pyaudio")
        try:
            import simpleaudio  # noqa
        except ImportError:
            missing.append("simpleaudio")
        if missing:
            st.error(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³: {', '.join(missing)}")
            st.info("`pip install " + " ".join(missing) + "`")
            return False
        st.success("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        return True

    def _show_settings(self):
        with st.expander("âš™ï¸ Realtime API è¨­å®š", expanded=False):
            st.selectbox("éŸ³å£°é¸æŠ",
                         config.get("audio.voices", ["alloy", "nova", "echo", "onyx", "shimmer"]),
                         key=f"realtime_voice_{self.safe_key}")
            st.selectbox("éŸ³å£°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ", ["pcm16", "g711_ulaw", "g711_alaw"], key=f"audio_format_{self.safe_key}")
            st.checkbox("éŸ³å£°æ¤œå‡º (VAD) æœ‰åŠ¹", value=True, key=f"vad_{self.safe_key}")
            st.info("è¨­å®šå®Œäº†å¾Œã€ã€Œãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")

    def _run_realtime_demo(self):
        try:
            with st.spinner("ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ä¸­..."):
                st.info("ğŸ”´ éŒ²éŸ³é–‹å§‹: ãƒã‚¤ã‚¯ã«å‘ã‹ã£ã¦è©±ã—ã¦ãã ã•ã„")
                st.info("â¹ï¸ çµ‚äº†ã™ã‚‹ã«ã¯ã€ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ã—ã¦ãã ã•ã„")
                asyncio.run(self._run_realtime_session())
        except Exception as e:
            self.handle_error(e)

    async def _run_realtime_session(self):
        try:
            import pyaudio
            import simpleaudio
            async with self.async_client.beta.realtime.connect(model="gpt-4o-realtime-preview") as conn:
                await conn.session.update(session={
                    "voice": "alloy",
                    "input_audio_format": "pcm16",
                    "input_audio_transcription": {"model": "gpt-4o-transcribe"},
                    "turn_detection": {"type": "server_vad"},
                })
                pa = pyaudio.PyAudio()
                stream = pa.open(format=pyaudio.paInt16, channels=1, rate=16_000, input=True, frames_per_buffer=1024)

                async def sender():
                    while True:
                        pcm = stream.read(1024, exception_on_overflow=False)
                        await conn.input_audio_buffer.append(audio=base64.b64encode(pcm).decode())

                async def receiver():
                    async for event in conn:
                        if event.type == "response.audio.delta":
                            wav = base64.b64decode(event.audio)
                            simpleaudio.play_buffer(wav, 1, 2, 24_000)
                        elif event.type == "response.done":
                            break

                await asyncio.gather(sender(), receiver())
        except Exception as e:
            logger.error(f"Realtime API error: {e}")
            raise e

# ==================================================
# Chained Voice Agent ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
class ChainedVoiceAgentDemo(BaseDemo):
    """Chained Voice Agent ã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        self.initialize()

        with st.expander("OpenAI Audio API:å®Ÿè£…ä¾‹", expanded=False):
            st.write("éŸ³å£°â†’ãƒ†ã‚­ã‚¹ãƒˆâ†’Chatâ†’éŸ³å£°ã®é€£é–å‡¦ç†ãƒ‡ãƒ¢ã€‚")
            st.code('''# Chained Voice Agentå‡¦ç†ä¾‹
def voice_agent_chain(audio_bytes):
    # 1. STT: éŸ³å£°â†’ãƒ†ã‚­ã‚¹ãƒˆ
    transcript = client.audio.transcriptions.create(
        model=stt_model,
        file=BytesIO(audio_bytes),
        response_format="text"
    )
    # 2. Chat: ãƒ†ã‚­ã‚¹ãƒˆâ†’å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    chat_response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=build_chat_messages(transcript)
    )
    # 3. TTS: å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆâ†’éŸ³å£°
    tts_response = client.audio.speech.create(
        model=tts_model,
        voice=voice,
        input=assistant_text
    )
    return tts_response.content''', language="python")

        col1, col2 = st.columns(2)
        with col1:
            stt_model = st.selectbox("STTãƒ¢ãƒ‡ãƒ«", ["whisper-1", "gpt-4o-transcribe"], key=f"stt_model_{self.safe_key}")
        with col2:
            tts_model = st.selectbox("TTSãƒ¢ãƒ‡ãƒ«", ["tts-1", "tts-1-hd", "gpt-4o-mini-tts"], key=f"tts_model_{self.safe_key}")
        voice = UIHelper.select_voice(f"voice_agent_{self.safe_key}")

        st.write("### ğŸ¤ éŸ³å£°å…¥åŠ›")
        uploaded_file = st.file_uploader("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (WAV/MP3 â‰¤25MB)",
                                         type=["wav", "mp3"],
                                         key=f"voice_upload_{self.safe_key}")
        if uploaded_file:
            st.audio(uploaded_file)  # å½¢å¼ã¯è‡ªå‹•åˆ¤å®š
            if st.button("ğŸ¤– Voice Agent å®Ÿè¡Œ", key=f"agent_{self.safe_key}"):
                self._process_voice_agent(uploaded_file, stt_model, tts_model, voice)

        st.write("### ğŸ“ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é¸æŠ")
        audio_files = self._get_audio_files()
        if audio_files:
            selected_file = st.selectbox("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
                                         [f.name for f in audio_files],
                                         key=f"existing_audio_{self.safe_key}")
            audio_file_path = DATA_DIR / selected_file
            st.audio(str(audio_file_path), format="audio/mp3")
            if st.button("ğŸ¤– æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã§Agentå®Ÿè¡Œ", key=f"agent_existing_{self.safe_key}"):
                with audio_file_path.open("rb") as f:
                    audio_bytes = f.read()
                self._process_voice_agent(audio_bytes, stt_model, tts_model, voice)

    def _get_audio_files(self) -> List[Path]:
        patterns = config.get("audio.supported_formats", ["*.mp3", "*.wav", "*.m4a"])
        files: List[Path] = []
        for pattern in patterns:
            files.extend(DATA_DIR.glob(pattern))
        return sorted(files)

    def _process_voice_agent(self, audio_input: Union[bytes, Any], stt_model: str, tts_model: str, voice: str):
        try:
            session_key = f"demo_state_{self.safe_key}"
            if session_key in st.session_state:
                st.session_state[session_key]['execution_count'] += 1

            if hasattr(audio_input, 'read'):
                audio_bytes = audio_input.read()
            else:
                audio_bytes = audio_input

            with st.spinner("ğŸ¤– Voice Agent å‡¦ç†ä¸­..."):
                status_placeholder = st.empty()
                status_placeholder.info("ğŸ¤ Step 1: éŸ³å£°èªè­˜ä¸­...")
                transcript = self._voice_agent_stt(audio_bytes, stt_model)
                status_placeholder.info("ğŸ’­ Step 2: å¿œç­”ç”Ÿæˆä¸­...")
                assistant_text = self._voice_agent_chat(transcript)
                status_placeholder.info("ğŸµ Step 3: éŸ³å£°åˆæˆä¸­...")
                response_audio = self._voice_agent_tts(assistant_text, tts_model, voice)
                status_placeholder.success("âœ… Voice Agent å‡¦ç†å®Œäº†")

            self._display_voice_agent_result(transcript, assistant_text, response_audio)
        except Exception as e:
            self.handle_error(e)

    def _voice_agent_stt(self, audio_bytes: bytes, model: str) -> str:
        try:
            def stt_call():
                return self.openai_client.audio.transcriptions.create(
                    model=model,
                    file=BytesIO(audio_bytes),
                    response_format="text",
                )
            result = self.safe_audio_api_call(stt_call)
            return result if isinstance(result, str) else getattr(result, "text", str(result))
        except Exception as e:
            logger.error(f"STT error: {e}")
            raise e

    def _voice_agent_chat(self, user_text: str) -> str:
        try:
            messages: List[ChatCompletionMessageParam] = [
                ChatCompletionSystemMessageParam(
                    role="system",
                    content="You are a helpful Japanese voice assistant. è¿”ç­”ã¯æ—¥æœ¬èªã§ã€2æ–‡ä»¥å†…ã€‚"
                ),
                ChatCompletionUserMessageParam(role="user", content=user_text),
            ]
            def chat_call():
                return self.openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages)
            response = self.safe_audio_api_call(chat_call)
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Chat error: {e}")
            raise e

    def _voice_agent_tts(self, text: str, model: str, voice: str) -> bytes:
        try:
            def tts_call():
                return self.openai_client.audio.speech.create(model=model, voice=voice, input=text)
            response = self.safe_audio_api_call(tts_call)
            return response.content
        except Exception as e:
            logger.error(f"TTS error: {e}")
            raise e

    def _display_voice_agent_result(self, transcript: str, assistant_text: str, response_audio: bytes):
        st.success("ğŸ¤– Voice Agent å¿œç­”å®Œäº†")
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ¤ ã‚ãªãŸã®ç™ºè©±")
            st.markdown(f"> {transcript}")
        with col2:
            st.subheader("ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”")
            st.markdown(f"> {assistant_text}")

        st.subheader("ğŸµ å¿œç­”éŸ³å£°")
        st.audio(response_audio, format="audio/mp3")

        ts = int(time.time())
        response_filename = f"voice_agent_response_{ts}.mp3"
        response_path = DATA_DIR / response_filename
        response_path.write_bytes(response_audio)
        st.info(f"ğŸ“ å¿œç­”éŸ³å£°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {response_filename}")
        UIHelper.create_audio_download_button(response_audio, response_filename, "ğŸ“¥ å¿œç­”éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

# ==================================================
# ãƒ‡ãƒ¢ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
class AudioDemoManager:
    """éŸ³å£°ãƒ‡ãƒ¢ã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    def __init__(self):
        self.config = ConfigManager("config.yml")
        self.demos = self._initialize_demos()

    def _initialize_demos(self) -> Dict[str, BaseDemo]:
        return {
            "Text to Speech": TextToSpeechDemo("Text to Speech"),
            "Speech to Text": SpeechToTextDemo("Speech to Text"),
            "Speech Translation": SpeechTranslationDemo("Speech Translation"),
            "Realtime API": RealtimeApiDemo("Realtime API"),
            "Chained Voice Agent": ChainedVoiceAgentDemo("Chained Voice Agent"),
        }

    @error_handler_ui
    @timer_ui
    def run(self):
        try:
            SessionStateManager.init_session_state()
            demo_name = st.sidebar.radio("ğŸµ Audio Demo ã‚’é¸æŠ", list(self.demos.keys()), key="audio_demo_selection")

            if "current_audio_demo" not in st.session_state:
                st.session_state.current_audio_demo = demo_name
            elif st.session_state.current_audio_demo != demo_name:
                st.session_state.current_audio_demo = demo_name

            demo = self.demos.get(demo_name)
            if demo:
                demo.run()
            else:
                st.error(f"ãƒ‡ãƒ¢ '{demo_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            self._display_footer()
        except Exception as e:
            st.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"Audio application execution error: {e}")
            if config.get("experimental.debug_mode", False):
                with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=False):
                    st.exception(e)

    def _display_footer(self):
        st.sidebar.markdown("---")
        st.sidebar.markdown("### ğŸµ Audio API æƒ…å ±")
        with st.sidebar.expander("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±"):
            audio_files = list(DATA_DIR.glob("*.mp3")) + list(DATA_DIR.glob("*.wav"))
            txt_files = list(DATA_DIR.glob("*.txt"))
            st.write(f"**ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª**: {DATA_DIR}")
            st.write(f"**éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {len(audio_files)}")
            st.write(f"**ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {len(txt_files)}")

        with st.sidebar.expander("ç¾åœ¨ã®è¨­å®š"):
            safe_streamlit_json({
                "data_directory": str(DATA_DIR),
                "audio_models": config.get("models.categories.audio"),
                "supported_formats": config.get("audio.supported_formats"),
                "debug_mode": config.get("experimental.debug_mode"),
            })

        st.sidebar.markdown("### ãƒãƒ¼ã‚¸ãƒ§ãƒ³")
        st.sidebar.markdown("- OpenAI Audio & Speech Demo v3.1 (çµ±ä¸€åŒ–ç‰ˆ)")
        st.sidebar.markdown("- Streamlit " + st.__version__)
        st.sidebar.markdown("### ãƒªãƒ³ã‚¯")
        st.sidebar.markdown("[OpenAI Audio API](https://platform.openai.com/docs/guides/speech-to-text)")
        st.sidebar.markdown("[Realtime API](https://platform.openai.com/docs/guides/realtime)")

# ==================================================
# ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
def main():
    try:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        if not os.getenv("OPENAI_API_KEY"):
            st.error("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            st.info("export OPENAI_API_KEY='your-api-key' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        SessionStateManager.init_session_state()
        manager = AudioDemoManager()
        manager.run()
    except Exception as e:
        st.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        logger.error(f"Audio application startup error: {e}")
        if config.get("experimental.debug_mode", False):
            with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=False):
                st.exception(e)

if __name__ == "__main__":
    main()

# streamlit run a04_audio_speeches.py --server.port=8504
